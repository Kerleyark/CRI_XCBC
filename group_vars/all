---
#OpenHPC release version
  openhpc_release_rpm: "https://github.com/openhpc/ohpc/releases/download/v1.3.GA/ohpc-release-1.3-1.el7.x86_64.rpm"
  openhpc_repo_tarball: "http://build.openhpc.community/dist/1.3.9/OpenHPC-1.3.9.CentOS_7.x86_64.tar"
#The full list of available releases can be viewed at
# https://github.com/openhpc/ohpc/releases/ 
#
# Headnode Info
  public_interface: "enp0s3" # NIC that allows access to the public internet
  private_interface: "enp0s9" #NIC that allows access to compute nodes
  headnode_private_ip: "10.0.0.1"

  headnode_public_ip: "192.168.56.4" #World-accessible IP for Ganglia, etc.
  build_kernel_ver: '3.10.0-1127.13.1.el7.x86_64' # `uname -r` at build time... for wwbootstrap
  headnode_dns_server: "10.0.2.3"
  dhcpd_enabled: true


#Provisioning System: True for Warewulf, False for xCAT
  use_warewulf: false
  CentOS_iso_url: "http://mirror.nodesdirect.com/centos/7.8.2003/isos/x86_64/CentOS-7-x86_64-DVD-2003.iso"
  CentOS_iso_version: "CentOS-7-x86_64-DVD-2003.iso"
  CentOS_checksums_url: "http://mirror.centos.org/centos/7/isos/x86_64/sha256sum.txt.asc"
  xCAT_core_repo: "https://xcat.org/files/xcat/repos/yum/latest/xcat-core/xcat-core.repo"
  xCAT_dep_repo: "https://xcat.org/files/xcat/repos/yum/xcat-dep/rh7/x86_64/xcat-dep.repo"
  ohpc_repo_dir: "/opt/ohpc/ohpc_repo"
  epel_repo_dir: "/opt/ohpc/epel_repo"


#Private network Info
  private_network: "10.0.0.0"  
  private_network_mask: "24"
  private_network_long_netmask: "255.255.255.0"
  compute_ip_minimum: "10.0.0.2"
  compute_ip_maximum: "10.0.0.255"
  gpu_ip_minimum: "10.0.0.128" #This could be more clever, like compute_ip_minimum + num_nodes...
  enable_compute_NAT: true

# Optional network vars
  TenGb_network: "10.0.2.0"
  TenGb_netmask: "255.255.255.0"
  ib_network: "10.3.1.0"
  ib_netmask: "255.255.255.0"
  headnode_ib_ip: "10.3.1.254"
  
  local_domain: "local"

#Ganglia specific variables
  ganglia_grid_name: "ExampleSite"
  ganglia_public_name: "ganglia.localdomain"

#Storage mount interface  
  mount_ip: "{{ headnode_ib_ip }}"


#slurm.conf variables
  cluster_name: "xcbc-example"
#  gres_types: "gpu"

#Stateful compute or not?
  stateful_nodes: false

#Node Config Vars - for stateful nodes
  sda1: "mountpoint=/boot:dev=sda1:type=ext3:size=500"
  sda2: "dev=sda2:type=swap:size=500"
  sda3: "mountpoint=/:dev=sda3:type=ext3:size=fill"

# GPU Node Vars
# download the nvidia cuda installer, and run with only --extract=$path_to_CRI_XCBC/roles/gpu_build_vnfs/files to get these three installers
  nvidia_driver_installer: "NVIDIA-Linux-x86_64-387.26.run"
  cuda_toolkit_installer: "cuda-linux.9.1.85-23083092.run"
  cuda_samples_installer: "cuda-samples.9.1.85-23083092-linux.run"
  nvidia_driver_args: "-k {{ kernel_ver.stdout }} --kernel-install-path=/lib/modules/{{ kernel_ver.stdout }}/kernel/drivers/video -z -s -X"
  cuda_toolkit_args: "--noprompt --prefix=/export/cuda"
  cuda_samples_args: "--noprompt --prefix=/export/cuda/samples --cudaprefix=/export/cuda"

# WW Template Names for wwmkchroot
  template_path: "/usr/libexec/warewulf/wwmkchroot/"
  compute_template: "compute-nodes"
  gpu_template: "gpu-nodes"  
  login_template: "login-nodes"  

# Chroot variables
  compute_chroot_loc: "/opt/ohpc/admin/images/{{ compute_chroot }}"
  compute_chroot: centos7-compute
  gpu_chroot_loc: "/opt/ohpc/admin/images/{{ gpu_chroot }}"
  gpu_chroot: centos7-gpu
  login_chroot_loc: "/opt/ohpc/admin/images/{{ login_chroot }}"
  login_chroot: centos7-login 

# Node Inventory method - true: automatic, or false: manual
# Setting this to 'false' uses the list provided in the compute_nodes list below to add
# nodes to the Warewulf databse. 'true' uses the wwnodescan utility to automatically
# detect and add nodes.
  node_inventory_auto: true

#Node naming variables - no need to change
  compute_node_prefix: "compute-"
  num_compute_nodes: 2
  gpu_node_prefix: "gpu-compute-"
  num_gpu_nodes: 1
  login_node_prefix: "login-"
  num_login_nodes: 0

#Node Inventory - not in the Ansible inventory sense! Just for WW and Slurm config.
# Someday I will need a role that can run wwnodescan, and add nodes to this file! Probably horrifying practice.
# There is a real difference between building from scratch, and using these for maintenance / node addition!
#
  compute_private_nic: "eth0"
  compute_nodes:
   - { name: "compute-0", vnfs: '{{compute_chroot}}',  cpus: 1, sockets: 1, corespersocket: 1,  mac: "08:00:27:D6:B7:45", ip: "10.0.0.2", bmc_ip: "10.0.1.2", bmc_username: "admin", bmc_password: "admin", TenGb_ip: "10.0.2.2"} 
   - { name: "compute-1", vnfs: '{{compute_chroot}}',  cpus: 1, sockets: 1, corespersocket: 1,  mac: "08:00:27:E5:28:88", ip: "10.0.0.3", bmc_ip: "10.0.1.3", bmc_username: "admin", bmc_password: "admin", TenGb_ip: "10.0.2.3"} 

  login_nodes:
   - { name: "login-1", vnfs: '{{login_chroot}}', cpus: 8, sockets: 2, corespersocket: 4,  mac: "00:26:b9:2e:21:ed", ip: "10.2.255.137"}
 
  gpu_nodes:
   - { name: "gpu-compute-1", vnfs: '{{gpu_chroot}}', gpus: 4, gpu_type: "gtx_TitanX", cpus: 16, sockets: 2, corespersocket: 8,  mac: "0c:c4:7a:6e:9d:6e", ip: "10.2.255.47"}
 
  viz_nodes:
   - { name: "viz-node-0-0", vnfs: '{{gpu_chroot}}', gpus: 2, gpu_type: nvidia_gtx_780, cpus: 8, sockets: 2, corespersocket: 4,  mac: "foo", ip: "bar"}

#Slurm Accounting Variables - little need to change these
  slurm_acct_db: "slurmdb"
  slurmdb_storage_port: "7031"
  slurmdb_port: "1234"
  slurmdb_sql_pass: "password" #could force this to be a hash... 
  slurmdb_sql_user: slurm

# Active Directory Integration
  enable_active_directory: false

#automatic variables for internal use
# Don't edit these!
  compute_node_glob: "{{ compute_node_prefix }}[0-{{ num_compute_nodes|int - 1}}]"
  gpu_node_glob: "{{ gpu_node_prefix }}[0-{{ num_gpu_nodes|int - 1}}]"
  compute_node_glob_bash: "{{ compute_node_prefix }}{0..{{ num_compute_nodes|int - 1}}}"
  gpu_node_glob_bash: "{{ gpu_node_prefix }}{0..{{ num_gpu_nodes|int - 1}}}"
  # to be deprecated:
  node_glob_bash: "{{ compute_node_prefix }}{0..{{ num_compute_nodes|int - 1}}}"
